# Diagnose & Fix Runtime Errors Implementation Plan

## Overview
This plan outlines the steps to systematically diagnose and fix runtime errors across the application. It focuses on improving error handling and propagation, enhancing the robustness of shared memory operations, and bolstering data quality and validation within the trading environment.

## Current State Analysis
Based on the research document (file: `/home/kamau/bsopt/26c2512b/research_2026-02-04.md`):
- **Generic Error Handling**: `src/streaming/consumer.py:_process_batch` lacks `return_exceptions=True` in `asyncio.gather`, leading to full batch failures on single message errors. `train_td3` in `src/ml/reinforcement_learning/train.py` has unhandled exceptions that can crash training.
- **Shared Memory (SHMManager) Fragility**: Potential for `PermissionError`/`OSError` during creation/access. Spin-lock can hang, and data corruption can occur with `msgspec` mismatches.
- **Data Quality in Trading Environment**: `TradingEnvironment` lacks explicit validation for numerical stability (e.g., `np.log` of non-positive values), leading to `NaN`/`inf` propagation.

## Implementation Approach
The implementation will proceed in three logical phases, focusing on increasing error visibility, system resilience, and data integrity.

## Phase 1: Improve Generic Error Handling & Propagation
### Overview
Ensure errors provide clearer, more specific context at their origin, especially for batch processing in the streaming consumer and critical ML training operations.

### Changes Required:
#### 1. `src/streaming/consumer.py`
**Changes**: Modify `_process_batch` to use `return_exceptions=True` with `asyncio.gather` and provide specific error reporting for failed messages within the batch.

```python
# Old:
# async def _process_batch(self, batch: List[Dict], callback: Callable):
#     """Process batch of messages in parallel using gather"""
#     start_time = time.time()
#     try:
#         tasks = [callback(msg) for msg in batch]
#         await asyncio.gather(*tasks)
#         
#         duration = time.time() - start_time
#         if duration <= 0:
#             duration = 0.001
#             
#         logger.info(
#             "batch_processed", 
#             batch_size=len(batch), 
#             duration_ms=duration * 1000,
#             throughput=len(batch) / duration
#         )
#     except Exception as e:
#         logger.error("batch_processing_error", error=str(e))

# New:
async def _process_batch(self, batch: List[Dict], callback: Callable):
    """Process batch of messages in parallel using gather"""
    start_time = time.time()
    
    tasks = [callback(msg) for msg in batch]
    # Use return_exceptions=True to allow individual task failures without stopping the whole batch
    results = await asyncio.gather(*tasks, return_exceptions=True)
    
    processed_count = 0
    failed_count = 0
    for i, res in enumerate(results):
        if isinstance(res, Exception):
            failed_count += 1
            logger.error("streaming_message_processing_failed", error=str(res), message=batch[i])
        else:
            processed_count += 1
    
    duration = time.time() - start_time
    if duration <= 0:
        duration = 0.001
        
    logger.info(
        "batch_processed_summary", 
        batch_size=len(batch),
        processed_ok=processed_count,
        failed=failed_count,
        duration_ms=duration * 1000,
        throughput=len(batch) / duration
    )

    if failed_count > 0:
        logger.warning("streaming_batch_partial_failure", failed_count=failed_count, total_count=len(batch))
```

#### 2. `src/ml/reinforcement_learning/train.py`
**Changes**: Add try-except blocks around critical operations in `train_td3` (environment setup, model learning, model saving) to catch common ML-specific errors and provide more informative logging.

```python
# Old:
# def train_td3(total_timesteps: int = 10000, model_path: str = "models/best_td3"):
#     mlflow.set_tracking_uri(settings.tracking_uri)
#     mlflow.set_experiment("rl_trading_singularity")
#     
#     with mlflow.start_run() as run:
#         env = TradingEnvironment()
#         eval_env = TradingEnvironment()
#         
#         policy_kwargs = dict(
#             features_extractor_class=TransformerSingularityExtractor,
#             features_extractor_kwargs=dict(features_dim=512, d_model=256, nhead=8, num_layers=4),
#             net_arch=dict(pi=[256, 256], qf=[256, 256])
#         )
#         # ... (rest of the old code)

# New:
def train_td3(total_timesteps: int = 10000, model_path: str = "models/best_td3"):
    mlflow.set_tracking_uri(settings.tracking_uri)
    mlflow.set_experiment("rl_trading_singularity")
    
    with mlflow.start_run() as run:
        try:
            env = TradingEnvironment()
            eval_env = TradingEnvironment()
        except Exception as e:
            logger.error("ml_env_setup_failed", error=str(e))
            raise # Re-raise to crash training if env fails
            
        policy_kwargs = dict(
            features_extractor_class=TransformerSingularityExtractor,
            features_extractor_kwargs=dict(features_dim=512, d_model=256, nhead=8, num_layers=4),
            net_arch=dict(pi=[256, 256], qf=[256, 256])
        )

        n_actions = env.action_space.shape[-1]
        action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma=0.1 * np.ones(n_actions))

        try:
            model = TD3(
                TransformerTD3Policy, 
                env, 
                action_noise=action_noise, 
                verbose=1,
                policy_kwargs=policy_kwargs,
                learning_rate=1e-4,
                buffer_size=200000,
                batch_size=256,
                tau=0.005,
                gamma=0.99
            )
        except Exception as e:
            logger.error("ml_model_init_failed", error=str(e))
            raise

        eval_callback = EvalCallback(
            eval_env, 
            best_model_save_path=model_path,
            log_path="./logs/results/", 
            eval_freq=max(1, total_timesteps // 10),
            deterministic=True
        )
        
        shm_callback = SHMWeightSyncCallback()
        callback = CallbackList([eval_callback, shm_callback])
        
        logger.info("training_started", total_timesteps=total_timesteps)
        try:
            model.learn(total_timesteps=total_timesteps, callback=callback)
        except Exception as e:
            logger.error("ml_model_learn_failed", error=str(e))
            raise
        
        try:
            os.makedirs(os.path.dirname(model_path), exist_ok=True)
            model.save(model_path)
            mlflow.pytorch.log_model(model.policy, "model")
        except Exception as e:
            logger.error("ml_model_save_failed", error=str(e))
            raise
        
        return {"run_id": run.info.run_id, "model_path": model_path}
```

### Success Criteria:
#### Automated:
- [x] Add new unit tests for `MarketDataConsumer`'s `_process_batch` that:
    - Verifies valid messages are processed when some messages in the batch raise exceptions.
    - Verifies logging contains `streaming_message_processing_failed` for failed messages and `batch_processed_summary` for the batch.
- [x] Add new unit tests for `train_td3` that:
    - Mocks `TradingEnvironment` to raise an exception during initialization and verifies `ml_env_setup_failed` is logged.
    - Mocks `model.learn` to raise an exception and verifies `ml_model_learn_failed` is logged.
#### Manual:
- [x] Simulate a malformed Kafka message (e.g., non-Avro compatible) in a local Kafka setup and verify that `MarketDataConsumer` logs `deserialization_error` but continues processing other messages in the batch.
- [x] Manually run `bs_cli.py train_transformer` and introduce a deliberate error (e.g., in `TradingEnvironment`) and verify that specific ML-related errors are logged instead of a generic traceback.

**Implementation Note**: Pause for manual confirmation after this phase.

## Phase 2: Enhance Shared Memory (SHMManager) Robustness
### Overview
Increase the resilience of shared memory operations against system-level issues and data corruption.

### Changes Required:
#### 1. `src/shared/shm_manager.py`
**Changes**:
- Add more robust error handling around `shared_memory.SharedMemory` creation and attachment in `create()`.
- Add logging to the spin-lock mechanism in `write()` when a lock is forced clear due to timeout.
- Add `try-except` around `msgspec.msgpack.Decoder(schema)` in `__init__` to catch schema-related errors early.

```python
# Old:
# from multiprocessing import shared_memory
# import msgspec
# import structlog
# from typing import Any, Optional, TypeVar, Generic, Type

# logger = structlog.get_logger(__name__)

# T = TypeVar("T")

# class SHMManager(Generic[T]):
#     # ... (existing __init__)
#     def create(self):
#         """Create the shared memory block."""
#         try:
#             self._shm = shared_memory.SharedMemory(name=self.name, create=True, size=self.size)
#             logger.info("shm_created", name=self.name, size=self.size)
#         except FileExistsError:
#             self._shm = shared_memory.SharedMemory(name=self.name)
#             logger.warning("shm_already_exists", name=self.name)

#     def write(self, data: T):
#         # ... (existing write code)
#         # ðŸš€ ATOMIC: Acquire spin-lock
#         import time
#         start = time.perf_counter()
#         while self._shm.buf[0] != 0:
#             if time.perf_counter() - start > 0.1: # 100ms timeout
#                 self._shm.buf[0] = 0 # Force clear if hung
#             pass
#         # ... (rest of write)

# New:
from multiprocessing import shared_memory
import msgspec
import structlog
from typing import Any, Optional, TypeVar, Generic, Type

logger = structlog.get_logger(__name__)

T = TypeVar("T")

class SHMManager(Generic[T]):
    """
    Manages a shared memory block for a specific data type.
    """

    def __init__(self, name: str, schema: Type[T], size: int = 10 * 1024 * 1024):
        self.name = name
        self.schema = schema
        self.size = size
        self._shm: Optional[shared_memory.SharedMemory] = None
        
        try: # Catch schema-related errors early
            self._encoder = msgspec.msgpack.Encoder()
            self._decoder = msgspec.msgpack.Decoder(schema)
        except Exception as e:
            logger.error("shm_msgspec_init_failed", name=name, schema=str(schema), error=str(e))
            raise
            
    def create(self):
        """Create the shared memory block."""
        try:
            self._shm = shared_memory.SharedMemory(name=self.name, create=True, size=self.size)
            logger.info("shm_created", name=self.name, size=self.size)
        except FileExistsError:
            try:
                self._shm = shared_memory.SharedMemory(name=self.name)
                logger.warning("shm_already_exists", name=self.name)
            except (PermissionError, OSError) as e:
                logger.error("shm_attach_failed", name=self.name, error=str(e))
                raise # Re-raise to prevent uninitialized SHM usage
        except (PermissionError, OSError) as e:
            logger.error("shm_create_failed", name=self.name, error=str(e))
            raise

    def write(self, data: T):
        """
        Write data to the shared memory block atomically.
        Uses first byte as a spin-lock and next 4 bytes for length header.
        """
        if not self._shm:
            raise RuntimeError("SHM not initialized.")
        
        packed = self._encoder.encode(data)
        if len(packed) > self.size - 5:
            raise ValueError(f"Data size {len(packed)} exceeds SHM capacity (max: {self.size - 5})")

        # ðŸš€ ATOMIC: Acquire spin-lock
        import time
        start = time.perf_counter()
        while self._shm.buf[0] != 0:
            if time.perf_counter() - start > 0.1: # 100ms timeout
                logger.warning("shm_spinlock_forced_clear", name=self.name, timeout_ms=100)
                self._shm.buf[0] = 0 # Force clear if hung
            pass
        
        self._shm.buf[0] = 1 # Lock
        try:
            # Write length (4 bytes)
            import struct
            self._shm.buf[1:5] = struct.pack("I", len(packed))
            # Write data
            self._shm.buf[5:5+len(packed)] = packed
        finally:
            self._shm.buf[0] = 0 # Unlock
            
        logger.debug("shm_write_atomic", name=self.name, bytes=len(packed))
```

### Success Criteria:
#### Automated:
- [x] Add new unit tests for `SHMManager.create()` that:
    - Simulates `PermissionError` during create and verifies it raises the error with correct logging.
    - Simulates `PermissionError` during attach (after `FileExistsError`) and verifies it raises the error with correct logging.
- [x] Add new unit tests for `SHMManager.write()` that:
    - Verifies `ValueError` is raised when data size exceeds SHM capacity.
    - Verifies `shm_spinlock_forced_clear` is logged if a write attempts to acquire a hung lock.
- [x] Add a unit test for `SHMManager.__init__` that passes an invalid schema to `msgspec.msgpack.Decoder` and verifies it raises an appropriate error with `shm_msgspec_init_failed` logged.
#### Manual:
- [x] Manually create a shared memory segment with restricted permissions (e.g., using a separate process) and verify that `SHMManager.create()` fails with a clear error message.
- [x] Introduce a deliberate delay in a process holding the SHM spin-lock and verify that a logger message `shm_spinlock_forced_clear` appears when another process attempts to write.
- [x] Modify a schema passed to `SHMManager` and try to write data that doesn't conform. Verify that `shm_msgspec_init_failed` is logged early.

**Implementation Note**: Pause for manual confirmation after this phase.

## Phase 3: Data Quality & Validation in Trading Environment
### Overview
Mitigate runtime errors caused by poor data quality, numerical instabilities, and inconsistent data formats in the `TradingEnvironment`.

### Changes Required:
#### 1. `src/ml/reinforcement_learning/trading_env.py`
**Changes**:
- Add explicit validation and logging for critical numerical operations in `_construct_single_obs` (e.g., ensure `prices` and `strikes` are positive before `np.log`). Handle `NaN`/`inf` propagation.
- Add validation for `current_prices` and other market data in `step()` before numerical operations.
- Add checks for division by zero (`prev_value`) and validate inputs to `np.diff`/`np.std` in `_calculate_reward`.

```python
# Existing imports:
# import gymnasium as gym
# from gymnasium import spaces
# import numpy as np
# from typing import Dict, Tuple
# import structlog

# logger = structlog.get_logger()

# class TradingEnvironment(gym.Env):
#     # ... (existing init)

#     def _construct_single_obs(self) -> np.ndarray:
#         """Constructs a single 100-dim observation vector."""
#         vec = np.zeros(100, dtype=np.float32)
#         # ... (existing portfolio state)
#         
#         # 2. Market prices (10 dimensions)
#         prices = np.array(self.market_data.get('prices', []))
#         strikes = np.array(self.market_data.get('strikes', prices))
#         n_prices = min(len(prices), 10)
#         if n_prices > 0:
#             vec[11:11+n_prices] = np.log(prices[:n_prices] / strikes[:n_prices])
#         # ... (rest of old _construct_single_obs)

#     def step(self, action: np.ndarray) -> Tuple[np.ndarray, float, bool, bool, Dict]:
#         # ... (existing clip action, trades, positions)
#         # 2. Calculate costs
#         current_prices = np.array(self.market_data.get('prices', np.zeros(10)))
#         if len(current_prices) != 10:
#             current_prices = np.pad(current_prices, (0, 10 - len(current_prices)))[:10]
#         # ... (rest of old step)

#     def _calculate_reward(self, current_portfolio_value: float) -> float:
#         """
#         Multi-objective reward function incorporating risk-adjusted returns.
#         Improved stability using larger window and downside risk focus.
#         """
#         # 1. Percentage return
#         prev_value = self.portfolio_values[-2]
#         ret = (current_portfolio_value - prev_value) / prev_value
#         # ... (rest of old _calculate_reward)


# New:
import gymnasium as gym
from gymnasium import spaces
import numpy as np
from typing import Dict, Tuple
import structlog

logger = structlog.get_logger(__name__)

class TradingEnvironment(gym.Env):
    # ... (existing init)

    def _construct_single_obs(self) -> np.ndarray:
        """Constructs a single 100-dim observation vector with added validation."""
        vec = np.zeros(100, dtype=np.float32)
        # 1. Portfolio state (11 dimensions)
        vec[0] = self.balance / self.initial_balance
        vec[1:11] = self.positions[:10]
        
        # 2. Market prices (10 dimensions)
        prices = np.array(self.market_data.get('prices', []))
        strikes = np.array(self.market_data.get('strikes', prices))
        
        # Validation for non-positive prices/strikes before log
        if np.any(prices <= 0) or np.any(strikes <= 0):
            logger.warning("trading_env_non_positive_price_or_strike", prices=prices, strikes=strikes)
            # Replace with a small positive value or handle as an error condition
            # For now, setting to a default small positive value to avoid NaN
            prices[prices <= 0] = 1e-6
            strikes[strikes <= 0] = 1e-6

        n_prices = min(len(prices), 10)
        if n_prices > 0:
            vec[11:11+n_prices] = np.log(prices[:n_prices] / strikes[:n_prices])
        
        # 3. Greeks (50 dimensions)
        greeks_raw = self.market_data.get('greeks', [])
        greeks_flat = np.array(greeks_raw).flatten()
        n_greeks = min(len(greeks_flat), 50)
        vec[21:21+n_greeks] = np.tanh(greeks_flat[:n_greeks])
        
        # 4. Indicators (20 dimensions)
        indicators = self.market_data.get('indicators', [])
        n_ind = min(len(indicators), 20)
        vec[71:71+n_ind] = indicators[:n_ind]
        
        # Check for NaN/Inf in the final vector
        if np.any(np.isnan(vec)) or np.any(np.isinf(vec)):
            logger.error("trading_env_obs_nan_or_inf", obs_vec=vec)
            # Handle by replacing with finite values or raising an error
            vec[np.isnan(vec)] = 0
            vec[np.isinf(vec)] = 0
            
        return vec

    def step(self, action: np.ndarray) -> Tuple[np.ndarray, float, bool, bool, Dict]:
        """
        Execute one step in the environment with data validation.
        """
        # 0. Clip action to valid range
        action = np.clip(action, self.action_space.low, self.action_space.high)
        
        # 1. Execute trades (rebalance to target positions)
        trades = action - self.positions
        
        # 2. Calculate costs
        current_prices = np.array(self.market_data.get('prices', np.zeros(10)))
        if len(current_prices) != 10:
            current_prices = np.pad(current_prices, (0, 10 - len(current_prices)))[:10]
        
        # Validate current_prices before use
        if np.any(current_prices <= 0):
            logger.warning("trading_env_step_non_positive_prices", prices=current_prices)
            # Handle by replacing with a small positive value or triggering truncation
            current_prices[current_prices <= 0] = 1e-6
            
        transaction_costs = np.sum(np.abs(trades) * current_prices * self.transaction_cost)
        asset_costs = np.sum(trades * current_prices)
        
        # 3. Update state
        self.positions = action
        self.balance -= (transaction_costs + asset_costs)
        
        # Check for unexpectedly low balance due to errors, beyond normal trading
        if self.balance < -1e9: # Arbitrary large negative to catch calculation errors
            logger.critical("trading_env_balance_catastrophic_negative", balance=self.balance)
            # Force truncation or reset if balance becomes absurdly negative
            return self._get_observation(), -100.0, True, True, {} # Huge penalty, terminate
            
        # 4. Advance time
        self.current_step += 1
        if self.data_provider:
            self.market_data = self.data_provider.get_data_at_step(self.current_step)
        else:
            self.market_data = self._get_dummy_data()
            
        # 5. Calculate portfolio value
        # Recalculate current_prices after advancing step as market_data might change
        current_prices = np.array(self.market_data.get('prices', np.zeros(10)))
        if len(current_prices) != 10:
            current_prices = np.pad(current_prices, (0, 10 - len(current_prices)))[:10]

        # Validate current_prices again
        if np.any(current_prices <= 0):
            logger.warning("trading_env_portfolio_non_positive_prices", prices=current_prices)
            current_prices[current_prices <= 0] = 1e-6
            
        option_values = np.sum(self.positions * current_prices)
        portfolio_value = self.balance + option_values
        self.portfolio_values.append(portfolio_value)
        
        # 6. Calculate reward (Sharpe-like risk adjusted return)
        reward = self._calculate_reward(portfolio_value)
        
        # 7. Check termination
        terminated = False
        if self.data_provider and self.current_step >= len(self.data_provider) - 1:
            terminated = True
        elif not self.data_provider and self.current_step >= 100:
            terminated = True
            
        # 8. Check truncation (Drawdown limit)
        truncated = bool(portfolio_value <= self.initial_balance * 0.5)
        
        info = {
            'portfolio_value': portfolio_value,
            'balance': self.balance,
            'positions': self.positions.copy(),
            'step': self.current_step
        }
        
        return self._get_observation(), reward, terminated, truncated, info

    def _calculate_reward(self, current_portfolio_value: float) -> float:
        """
        Multi-objective reward function incorporating risk-adjusted returns with added validation.
        Improved stability using larger window and downside risk focus.
        """
        # 1. Percentage return
        prev_value = self.portfolio_values[-2]
        if prev_value == 0: # Avoid division by zero
            logger.error("trading_env_reward_div_by_zero", prev_value=prev_value, current_value=current_portfolio_value)
            return -100.0 # Huge penalty
            
        ret = (current_portfolio_value - prev_value) / prev_value
        
        # 2. Volatility penalty (risk adjustment) - Increased window to 20 for stability
        if len(self.portfolio_values) > 20:
            recent_values = np.array(self.portfolio_values[-20:])
            # Validate recent_values before np.diff
            if np.any(recent_values <= 0):
                logger.warning("trading_env_reward_non_positive_recent_values", values=recent_values)
                recent_values[recent_values <= 0] = 1e-6 # Replace to avoid issues
            
            recent_returns = np.diff(recent_values) / recent_values[:-1]
            
            # Check for NaN/Inf in recent_returns
            if np.any(np.isnan(recent_returns)) or np.any(np.isinf(recent_returns)):
                logger.error("trading_env_reward_returns_nan_or_inf", returns=recent_returns)
                volatility = 1.0 # High penalty for instability
            else:
                volatility = np.std(recent_returns)
                
            vol_penalty = 0.5 * volatility # Increased weight on risk
        else:
            vol_penalty = 0
            
        reward = ret - vol_penalty
        
        # 3. Progressive Drawdown penalty
        drawdown = (self.initial_balance - current_portfolio_value) / self.initial_balance
        if drawdown > 0.1: # 10% threshold
            reward -= 0.1 * (drawdown - 0.1)
            
        return float(reward)
```

### Success Criteria:
#### Automated:
- [ ] Add new unit tests for `TradingEnvironment._construct_single_obs` that:
    - Verifies handling of non-positive `prices` or `strikes` (e.g., replacement with `1e-6`) and associated warnings.
    - Verifies handling of `NaN`/`inf` in intermediate calculations (e.g., replacement with `0`) and associated errors.
- [ ] Add new unit tests for `TradingEnvironment.step()` that:
    - Simulates non-positive `current_prices` and verifies handling and warnings.
    - Simulates an absurdly negative `balance` and verifies early termination with a penalty.
- [ ] Add new unit tests for `TradingEnvironment._calculate_reward` that:
    - Simulates `prev_value == 0` and verifies a large negative penalty and error logging.
    - Simulates `NaN`/`inf` in `recent_returns` and verifies a high volatility penalty and error logging.
#### Manual:
- [ ] Debug a running `TradingEnvironment` (e.g., in a `train_td3` run) and deliberately introduce malformed market data (e.g., zero prices). Verify that environment warnings/errors are logged and handled gracefully without crashing the entire training.

**Implementation Note**: Pause for manual confirmation after this phase.
