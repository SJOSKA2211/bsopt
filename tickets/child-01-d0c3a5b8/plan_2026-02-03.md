# Dockerize Backend Services Implementation Plan

## Overview
This plan details the steps to containerize the `bsopt` backend services: `api`, `neural-pricing`, `scraper`, and `worker`. The primary goal is to adapt existing Dockerfiles for a consistent development environment, resolving identified inconsistencies in service entry points, and ensuring all services can be built and run effectively within Docker containers.

## Current State Analysis
(Referenced from research_2026-02-03.md)
- Existing Dockerfiles (`docker/Dockerfile.api`, `docker/Dockerfile.neural-pricing`, `docker/Dockerfile.scraper`, `docker/Dockerfile.worker`) utilize multi-stage builds (`python:3.12-slim`).
- Common build steps include installing build tools, creating a venv, and installing `requirements.txt`.
- `Dockerfile.api` (Uvicorn: `src.api.main:app`) and `Dockerfile.scraper` (Python: `src/scrapers/engine.py`) use non-root users and `tini`.
- `Dockerfile.neural-pricing` uses Uvicorn: `src.pricing.main:app`. (`src/pricing/main.py` verified to exist).
- **Inconsistency**: `Dockerfile.worker` uses Celery: `-A src.ml.celery_app`, while `src/tasks/celery_app.py` appears to be the more comprehensive worker entry point for development. This will be adjusted in the development Dockerfile.
- All services depend on `requirements.txt`.

## Implementation Approach
The approach will involve creating or modifying Dockerfiles for each service specifically for a *development* context, leveraging existing patterns where appropriate. Key differences will include:
1.  **Development-focused Dockerfiles**: Minimal optimization, focus on quick rebuilds and debugging capabilities (though actual debugging configuration will be in separate tickets).
2.  **Resolution of Inconsistencies**: Specifically addressing the worker entry point.
3.  **No production hardening**: Non-root user setup and `tini` will be omitted from dev Dockerfiles for simplicity unless critical for basic functionality, as production hardening is out of scope for this ticket.

## Phase 1: Dockerfile for API Service (`bsopt-api`)
### Overview
Create a development-optimized Dockerfile for the `api` service, based on the existing `docker/Dockerfile.api`. The primary goal is to ensure the `api` service can be built and run in a container, suitable for local development.
### Changes Required:
#### 1. `docker/Dockerfile.api-dev` (New file)
**Changes**: Create a new Dockerfile for development. Copy requirements and install dependencies. Copy the entire `src` directory to enable local volume mounting in `docker-compose`.
```dockerfile
# Development Dockerfile for the API Service
FROM python:3.12-slim

WORKDIR /app

# Install build dependencies for requirements.txt
RUN apt-get update && apt-get install -y --no-install-recommends \
    build-essential \
    libpq-dev \
    gcc \
    git \
    && rm -rf /var/lib/apt/lists/*

# Create and activate virtual environment
RUN python -m venv /opt/venv
ENV PATH="/opt/venv/bin:$PATH"

# Copy and install Python dependencies
COPY requirements.txt .
RUN pip install --default-timeout=1000 --no-cache-dir -r requirements.txt

# Copy the entire source directory for development
COPY src/ ./src
COPY pyproject.toml . # Needed for some local package references

ENV PYTHONDONTWRITEBYTECODE=1
ENV PYTHONUNBUFFERED=1

EXPOSE 8000

CMD ["uvicorn", "src.api.main:app", "--host", "0.0.0.0", "--port", "8000", "--reload", "--no-access-log"]
```
### Success Criteria:
#### Automated:
- [ ] Docker image `bsopt-api-dev` builds successfully (`docker build -f docker/Dockerfile.api-dev -t bsopt-api-dev .`).
#### Manual:
- [ ] Run `docker run -p 8000:8000 bsopt-api-dev` and verify the FastAPI app starts without errors.
- [ ] Access `http://localhost:8000/docs` in a browser to confirm API documentation is available.

**Implementation Note**: Pause for manual confirmation after this phase.

## Phase 2: Dockerfile for Neural Pricing Service (`bsopt-neural-pricing`)
### Overview
Create a development-optimized Dockerfile for the `neural-pricing` service, based on the existing `docker/Dockerfile.neural-pricing`. Ensure it uses the correct entry point and is suitable for local development.
### Changes Required:
#### 1. `docker/Dockerfile.neural-pricing-dev` (New file)
**Changes**: Create a new Dockerfile for development. Similar to `api-dev`, copy requirements, install dependencies, and copy the entire `src` directory.
```dockerfile
# Development Dockerfile for the Neural Pricing Service
FROM python:3.12-slim

WORKDIR /app

# Install build dependencies for requirements.txt
RUN apt-get update && apt-get install -y --no-install-recommends \
    build-essential \
    libpq-dev \
    gcc \
    git \
    && rm -rf /var/lib/apt/lists/*

# Create and activate virtual environment
RUN python -m venv /opt/venv
ENV PATH="/opt/venv/bin:$PATH"

# Copy and install Python dependencies
COPY requirements.txt .
RUN pip install --default-timeout=1000 --no-cache-dir -r requirements.txt

# Copy the entire source directory for development
COPY src/ ./src
COPY pyproject.toml . # Needed for some local package references

ENV PYTHONDONTWRITEBYTECODE=1
ENV PYTHONUNBUFFERED=1

EXPOSE 8000

CMD ["uvicorn", "src.pricing.main:app", "--host", "0.0.0.0", "--port", "8000", "--reload"]
```
### Success Criteria:
#### Automated:
- [ ] Docker image `bsopt-neural-pricing-dev` builds successfully (`docker build -f docker/Dockerfile.neural-pricing-dev -t bsopt-neural-pricing-dev .`).
#### Manual:
- [ ] Run `docker run -p 8001:8000 bsopt-neural-pricing-dev` (using a different external port to avoid conflicts) and verify the FastAPI app starts without errors.
- [ ] Access `http://localhost:8001/docs` in a browser to confirm API documentation is available.

**Implementation Note**: Pause for manual confirmation after this phase.

## Phase 3: Dockerfile for Scraper Service (`bsopt-scraper`)
### Overview
Create a development-optimized Dockerfile for the `scraper` service. This will include necessary browser dependencies for Playwright and ensure the service can be run in a container.
### Changes Required:
#### 1. `docker/Dockerfile.scraper-dev` (New file)
**Changes**: Create a new Dockerfile for development. This will be similar to the existing `Dockerfile.scraper` but adapted for development. Copy requirements, install dependencies, including Playwright. Copy the entire `src` directory.
```dockerfile
# Development Dockerfile for the Scraper Service
FROM python:3.12-slim

WORKDIR /app

# Install build dependencies for requirements.txt and Playwright
RUN apt-get update && apt-get install -y --no-install-recommends \
    build-essential \
    libpq-dev \
    gcc \
    git \
    curl \
    # Playwright system dependencies
    libnss3 \
    libnspr4 \
    libatk1.0-0 \
    libatk-bridge2.0-0 \
    libcups2 \
    libdrm2 \
    libxkbcommon0 \
    libxcomposite1 \
    libxdamage1 \
    libxfixes3 \
    libxrandr2 \
    libgbm1 \
    libasound2 \
    && rm -rf /var/lib/apt/lists/*

# Create and activate virtual environment
RUN python -m venv /opt/venv
ENV PATH="/opt/venv/bin:$PATH"

# Copy and install Python dependencies, including playwright and structlog
COPY requirements.txt .
RUN pip install --default-timeout=1000 --no-cache-dir -r requirements.txt \
    && pip install --default-timeout=1000 --no-cache-dir playwright structlog

# Install browsers for Playwright (to root's cache)
RUN playwright install chromium
RUN playwright install-deps chromium

# Copy the entire source directory for development
COPY src/ ./src
COPY pyproject.toml . # Needed for some local package references

ENV PYTHONDONTWRITEBYTECODE=1
ENV PYTHONUNBUFFERED=1
ENV PYTHONOPTIMIZE=2

CMD ["python", "src/scrapers/engine.py"]
```
### Success Criteria:
#### Automated:
- [ ] Docker image `bsopt-scraper-dev` builds successfully (`docker build -f docker/Dockerfile.scraper-dev -t bsopt-scraper-dev .`).
#### Manual:
- [ ] Run `docker run bsopt-scraper-dev` and verify the scraper starts and attempts to run (e.g., no immediate crashes). (Note: Full functionality may require other services like a database).

**Implementation Note**: Pause for manual confirmation after this phase.

## Phase 4: Dockerfile for Worker Service (`bsopt-worker`)
### Overview
Create a development-optimized Dockerfile for the `worker` service, resolving the entry point inconsistency. This Dockerfile will ensure the Celery worker starts correctly, pointing to `src.tasks.celery_app` for development.
### Changes Required:
#### 1. `docker/Dockerfile.worker-dev` (New file)
**Changes**: Create a new Dockerfile for development. Copy requirements, install dependencies. Copy the entire `src` directory. Most importantly, change the `CMD` to point to `src.tasks.celery_app`.
```dockerfile
# Development Dockerfile for the Worker Service
FROM python:3.12-slim

WORKDIR /app

# Install build dependencies for requirements.txt
RUN apt-get update && apt-get install -y --no-install-recommends \
    build-essential \
    libpq-dev \
    gcc \
    git \
    && rm -rf /var/lib/apt/lists/*

# Create and activate virtual environment
RUN python -m venv /opt/venv
ENV PATH="/opt/venv/bin:$PATH"

# Copy and install Python dependencies
COPY requirements.txt .
RUN pip install --default-timeout=1000 --no-cache-dir -r requirements.txt

# Copy the entire source directory for development
COPY src/ ./src
COPY pyproject.toml . # Needed for some local package references

ENV PYTHONDONTWRITEBYTECODE=1
ENV PYTHONUNBUFFERED=1

# Corrected CMD to use src.tasks.celery_app for development
CMD ["celery", "-A", "src.tasks.celery_app", "worker", "-l", "info"]
```
### Success Criteria:
#### Automated:
- [ ] Docker image `bsopt-worker-dev` builds successfully (`docker build -f docker/Dockerfile.worker-dev -t bsopt-worker-dev .`).
#### Manual:
- [ ] Run `docker run bsopt-worker-dev` and verify the Celery worker starts, connects to a broker (if available), and doesn't crash immediately. (Note: Full functionality requires a running Celery broker like Redis or RabbitMQ).

**Implementation Note**: Pause for manual confirmation after this phase.
