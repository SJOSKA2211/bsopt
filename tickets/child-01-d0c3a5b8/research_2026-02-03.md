# Research: Dockerize Backend Services (API, Neural Pricing, Scraper, Worker)

**Date**: 2026-02-03

## 1. Executive Summary
This research investigates the existing Dockerfiles and source code structure for the `bsopt` backend services: `api`, `neural-pricing`, `scraper`, and `worker`. The goal is to document their current state and dependencies to inform the containerization effort for a consistent development environment.

## 2. Technical Context
- **Existing Dockerfiles**:
    - `docker/Dockerfile.api`
    - `docker/Dockerfile.neural-pricing`
    - `docker/Dockerfile.scraper`
    - `docker/Dockerfile.worker`
    All follow a multi-stage build pattern using `python:3.12-slim` as the base image. Common steps include installing `build-essential`, `libpq-dev`, `gcc`, `git`, creating a Python virtual environment, and installing dependencies from `requirements.txt`.
    `Dockerfile.api` and `Dockerfile.scraper` implement non-root users and `tini`.
    `Dockerfile.api` exposes port 8000 and uses `uvicorn src.api.main:app`.
    `Dockerfile.neural-pricing` exposes port 8000 and uses `uvicorn src.pricing.main:app`. (Note: This refers to `src.pricing.main:app`, not `src.ml.main:app` as might be inferred from the `src/ml` directory, which is another point of inconsistency to investigate later for `neural-pricing` service)
    `Dockerfile.scraper` runs `python src/scrapers/engine.py`.
    `Dockerfile.worker` runs `celery -A src.ml.celery_app worker -l info`.

- **Service Source Code Structures**:
    - **`src/api`**: Located at `/home/kamau/bsopt/src/api`. Contains `main.py` (likely FastAPI entry point), `graphql` definitions, `routes`, `schemas`, and middleware.
    - **`src/ml` (for Neural Pricing)**: Located at `/home/kamau/bsopt/src/ml`. A substantial module containing subdirectories like `architectures`, `evaluation`, `feature_store`, `forecasting`, `reinforcement_learning`, `serving`, `training`, `utils`. `celery_app.py` and `main.py` are present directly under `src/ml`. The `Dockerfile.neural-pricing` references `src.pricing.main:app`, suggesting the `neural-pricing` service is defined within `src/pricing` and not `src/ml` directly. This needs further clarification during the implementation phase.
    - **`src/scrapers`**: Located at `/home/kamau/bsopt/src/scrapers`. Contains `engine.py`, which is the entry point for the scraper service.
    - **`src/tasks` (for Worker)**: Located at `/home/kamau/bsopt/src/tasks`. Appears to be the primary Celery worker application with `celery_app.py` and various task modules (`audit_tasks.py`, `data_tasks.py`, `ml_tasks.py`, `pricing_tasks.py`, `security_tasks.py`, `trading_tasks.py`). The `Dockerfile.worker` currently points to `src.ml.celery_app`, which is inconsistent with the presence of `src/tasks/celery_app.py`. This discrepancy needs to be resolved.

## 3. Findings & Analysis
- Existing Dockerfiles provide a solid base for containerization, utilizing multi-stage builds and some security hardening (non-root users).
- Inconsistencies exist between the `Dockerfile.neural-pricing` `CMD` (`src.pricing.main:app`) and the identified `src/ml` directory (which also contains `main.py` and `celery_app.py`). Further investigation during implementation will be needed to confirm the correct entry point and context for the `neural-pricing` service.
- A significant inconsistency exists for the `worker` service: `Dockerfile.worker` refers to `src.ml.celery_app`, while a `celery_app.py` is present in `src/tasks`. The `src/tasks` directory seems to contain more general-purpose tasks. This needs to be clarified and corrected to ensure the correct Celery worker is run.
- All backend services are Python-based, primarily using `requirements.txt` for dependencies.

## 4. Technical Constraints
- Reliance on `requirements.txt` for Python dependencies.
- Need to resolve the conflicting entry points for `neural-pricing` and `worker` services when adapting for a development environment.
- The `Dockerfile.scraper` includes `playwright install chromium` and related dependencies, requiring a larger image and specific handling for browsers.

## 5. Architecture Documentation
- Backend services are structured as independent Python applications.
- FastAPI is used for `api` and `neural-pricing` (assuming `uvicorn` entry points).
- Celery is used for background tasks (`worker`).
- Playwright is used for `scrapers`.
